{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54d56a9b",
   "metadata": {},
   "source": [
    "# 人脸数据采集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ccb8642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " enter user id end press <return> ==>  wwb\n",
      "\n",
      " [INFO] Initializing face capture. Look the camera and wait ...\n",
      "\n",
      " [INFO] Exiting Program and cleanup stuff\n"
     ]
    }
   ],
   "source": [
    "import paramiko\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "#先自动开启ssh，启动摄像头\n",
    "# SSH连接信息\n",
    "hostname = '192.168.137.32'  # 树莓派的IP地址\n",
    "port = 22  # SSH端口号，默认为22\n",
    "username = 'wwb2'  # 树莓派的用户名\n",
    "password = 'pi'  # 树莓派用户的密码\n",
    "\n",
    "# 创建SSH客户端对象\n",
    "client = paramiko.SSHClient()\n",
    "client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "\n",
    "try:\n",
    "    # 连接到树莓派\n",
    "    client.connect(hostname, port, username, password)\n",
    "    \n",
    "    # 执行sudo motion命令\n",
    "    stdin, stdout, stderr = client.exec_command('sudo motion')\n",
    "\n",
    "    # 获取命令执行结果\n",
    "    output = stdout.read().decode('utf-8')\n",
    "    print(output)\n",
    "    \n",
    "    # 关闭SSH连接\n",
    "    client.close()\n",
    "\n",
    "except paramiko.AuthenticationException:\n",
    "    print(\"认证失败，请检查用户名和密码。\")\n",
    "\n",
    "except paramiko.SSHException as ssh_err:\n",
    "    print(\"SSH连接错误: \", ssh_err)\n",
    "\n",
    "except paramiko.Exception as e:\n",
    "    print(\"错误: \", e)\n",
    "\n",
    "\n",
    "# 定义视频源的URL\n",
    "url = \"http://192.168.137.32:8081/\"#树莓派1，用于面部识别\n",
    "# url = \"http://mirror.aarnet.edu.au/pub/TED-talks/911Mothers_2010W-480p.mp4\"#测试视频用\n",
    "\n",
    "# 创建视频捕获对象\n",
    "cam = cv2.VideoCapture(url)\n",
    "cam.set(3, 640) # set video width\n",
    "cam.set(4, 480) # set video height\n",
    "\n",
    "face_detector = cv2.CascadeClassifier('Cascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# For each person, enter one numeric face id\n",
    "face_id = input('\\n enter user id end press <return> ==>  ')\n",
    "\n",
    "print(\"\\n [INFO] Initializing face capture. Look the camera and wait ...\")\n",
    "# Initialize individual sampling face count\n",
    "count = 0\n",
    "\n",
    "while(True):\n",
    "\n",
    "    ret, img = cam.read()\n",
    "    img = cv2.flip(img, -1) # flip video image vertically\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_detector.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "\n",
    "        cv2.rectangle(img, (x,y), (x+w,y+h), (255,0,0), 2)     \n",
    "        count += 1\n",
    "\n",
    "        # Save the captured image into the datasets folder\n",
    "        cv2.imwrite(\"dataset/User.\" + str(face_id) + '.' + str(count) + \".jpg\", gray[y:y+h,x:x+w])\n",
    "\n",
    "        cv2.imshow('image', img)\n",
    "\n",
    "    k = cv2.waitKey(100) & 0xff # Press 'ESC' for exiting video\n",
    "    if k == 27:\n",
    "        break\n",
    "    elif count >= 30: # Take 30 face sample and stop video\n",
    "         break\n",
    "\n",
    "# Do a bit of cleanup\n",
    "print(\"\\n [INFO] Exiting Program and cleanup stuff\")\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7425cba8",
   "metadata": {},
   "source": [
    "# 人脸训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d63f82c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "compute_face_descriptor(): incompatible function arguments. The following argument types are supported:\n    1. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], face: _dlib_pybind11.full_object_detection, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vector\n    2. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], num_jitters: int = 0) -> _dlib_pybind11.vector\n    3. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], faces: _dlib_pybind11.full_object_detections, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectors\n    4. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: List[numpy.ndarray[(rows,cols,3),numpy.uint8]], batch_faces: List[_dlib_pybind11.full_object_detections], num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectorss\n    5. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: List[numpy.ndarray[(rows,cols,3),numpy.uint8]], num_jitters: int = 0) -> _dlib_pybind11.vectors\n\nInvoked with: <_dlib_pybind11.face_recognition_model_v1 object at 0x000002BC479B2B70>, array([[[131,  34,  12],\n        [129,  32,  10],\n        [125,  31,  10],\n        ...,\n        [131,  34,  12],\n        [131,  34,  14],\n        [129,  32,  12]],\n\n       [[131,  34,  12],\n        [130,  33,  11],\n        [126,  32,  11],\n        ...,\n        [132,  35,  13],\n        [132,  35,  15],\n        [131,  34,  14]],\n\n       [[131,  34,  12],\n        [129,  32,  10],\n        [127,  33,  10],\n        ...,\n        [131,  32,  11],\n        [133,  34,  15],\n        [133,  34,  15]],\n\n       ...,\n\n       [[ 47,  41,  46],\n        [ 43,  36,  47],\n        [ 41,  34,  45],\n        ...,\n        [188,  82,  33],\n        [188,  82,  33],\n        [187,  81,  32]],\n\n       [[ 47,  40,  49],\n        [ 41,  34,  44],\n        [ 37,  31,  39],\n        ...,\n        [190,  81,  35],\n        [188,  82,  35],\n        [189,  83,  36]],\n\n       [[ 45,  38,  48],\n        [ 41,  34,  44],\n        [ 36,  30,  37],\n        ...,\n        [191,  82,  37],\n        [190,  84,  38],\n        [190,  84,  38]]], dtype=uint8), <_dlib_pybind11.full_object_detection object at 0x000002BC4F8DC7F0>, 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 61\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Find all the faces and face encodings in the current frame of video\u001b[39;00m\n\u001b[0;32m     60\u001b[0m face_locations \u001b[38;5;241m=\u001b[39m face_recognition\u001b[38;5;241m.\u001b[39mface_locations(rgb_small_frame)\n\u001b[1;32m---> 61\u001b[0m face_encodings \u001b[38;5;241m=\u001b[39m \u001b[43mface_recognition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mface_encodings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_small_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mface_locations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m face_names \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m face_encoding \u001b[38;5;129;01min\u001b[39;00m face_encodings:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# See if the face is a match for the known face(s)\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\face_recognition\\api.py:214\u001b[0m, in \u001b[0;36mface_encodings\u001b[1;34m(face_image, known_face_locations, num_jitters, model)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03mGiven an image, return the 128-dimension face encoding for each face in the image.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m:return: A list of 128-dimensional face encodings (one for each face in the image)\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    213\u001b[0m raw_landmarks \u001b[38;5;241m=\u001b[39m _raw_face_landmarks(face_image, known_face_locations, model)\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [np\u001b[38;5;241m.\u001b[39marray(face_encoder\u001b[38;5;241m.\u001b[39mcompute_face_descriptor(face_image, raw_landmark_set, num_jitters)) \u001b[38;5;28;01mfor\u001b[39;00m raw_landmark_set \u001b[38;5;129;01min\u001b[39;00m raw_landmarks]\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\face_recognition\\api.py:214\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03mGiven an image, return the 128-dimension face encoding for each face in the image.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m:return: A list of 128-dimensional face encodings (one for each face in the image)\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    213\u001b[0m raw_landmarks \u001b[38;5;241m=\u001b[39m _raw_face_landmarks(face_image, known_face_locations, model)\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [np\u001b[38;5;241m.\u001b[39marray(\u001b[43mface_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_face_descriptor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_landmark_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_jitters\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m raw_landmark_set \u001b[38;5;129;01min\u001b[39;00m raw_landmarks]\n",
      "\u001b[1;31mTypeError\u001b[0m: compute_face_descriptor(): incompatible function arguments. The following argument types are supported:\n    1. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], face: _dlib_pybind11.full_object_detection, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vector\n    2. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], num_jitters: int = 0) -> _dlib_pybind11.vector\n    3. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], faces: _dlib_pybind11.full_object_detections, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectors\n    4. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: List[numpy.ndarray[(rows,cols,3),numpy.uint8]], batch_faces: List[_dlib_pybind11.full_object_detections], num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectorss\n    5. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: List[numpy.ndarray[(rows,cols,3),numpy.uint8]], num_jitters: int = 0) -> _dlib_pybind11.vectors\n\nInvoked with: <_dlib_pybind11.face_recognition_model_v1 object at 0x000002BC479B2B70>, array([[[131,  34,  12],\n        [129,  32,  10],\n        [125,  31,  10],\n        ...,\n        [131,  34,  12],\n        [131,  34,  14],\n        [129,  32,  12]],\n\n       [[131,  34,  12],\n        [130,  33,  11],\n        [126,  32,  11],\n        ...,\n        [132,  35,  13],\n        [132,  35,  15],\n        [131,  34,  14]],\n\n       [[131,  34,  12],\n        [129,  32,  10],\n        [127,  33,  10],\n        ...,\n        [131,  32,  11],\n        [133,  34,  15],\n        [133,  34,  15]],\n\n       ...,\n\n       [[ 47,  41,  46],\n        [ 43,  36,  47],\n        [ 41,  34,  45],\n        ...,\n        [188,  82,  33],\n        [188,  82,  33],\n        [187,  81,  32]],\n\n       [[ 47,  40,  49],\n        [ 41,  34,  44],\n        [ 37,  31,  39],\n        ...,\n        [190,  81,  35],\n        [188,  82,  35],\n        [189,  83,  36]],\n\n       [[ 45,  38,  48],\n        [ 41,  34,  44],\n        [ 36,  30,  37],\n        ...,\n        [191,  82,  37],\n        [190,  84,  38],\n        [190,  84,  38]]], dtype=uint8), <_dlib_pybind11.full_object_detection object at 0x000002BC4F8DC7F0>, 1"
     ]
    }
   ],
   "source": [
    "# 摄像头头像识别\n",
    "import face_recognition\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# This is a demo of running face recognition on live video from your webcam. It's a little more complicated than the\n",
    "# other example, but it includes some basic performance tweaks to make things run a lot faster:\n",
    "#   1. Process each video frame at 1/4 resolution (though still display it at full resolution)\n",
    "#   2. Only detect faces in every other frame of video.\n",
    "\n",
    "# PLEASE NOTE: This example requires OpenCV (the `cv2` library) to be installed only to read from your webcam.\n",
    "# OpenCV is *not* required to use the face_recognition library. It's only required if you want to run this\n",
    "# specific demo. If you have trouble installing it, try any of the other demos that don't require it instead.\n",
    "\n",
    "# 定义视频源的URL\n",
    "# url = \"http://192.168.137.32:8081/\"#树莓派1，用于面部识别\n",
    "url = \"http://mirror.aarnet.edu.au/pub/TED-talks/911Mothers_2010W-480p.mp4\"#测试视频用\n",
    "video_capture = cv2.VideoCapture(url)\n",
    "\n",
    "# Load a sample picture and learn how to recognize it.\n",
    "lwz_image = face_recognition.load_image_file(\"lwz.jpg\")\n",
    "lwz_face_encoding = face_recognition.face_encodings(lwz_image)[0]\n",
    "\n",
    "# Load a second sample picture and learn how to recognize it.\n",
    "wwb_image = face_recognition.load_image_file(\"wwb.png\")\n",
    "wwb_face_encoding = face_recognition.face_encodings(wwb_image)[0]\n",
    "\n",
    "# Create arrays of known face encodings and their names\n",
    "known_face_encodings = [\n",
    "    lwz_face_encoding,\n",
    "    wwb_face_encoding\n",
    "]\n",
    "known_face_names = [\n",
    "    \"Ling Wuzhen\",\n",
    "    \"Wang Weibo\"\n",
    "]\n",
    "\n",
    "# Initialize some variables\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "process_this_frame = True\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    # Only process every other frame of video to save time\n",
    "    if process_this_frame:\n",
    "        # Resize frame of video to 1/4 size for faster face recognition processing\n",
    "        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "\n",
    "        # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "        rgb_small_frame = small_frame[:, :, ::-1]\n",
    "        \n",
    "        face_locations=[]\n",
    "        face_encodings=[]\n",
    "        \n",
    "        # Find all the faces and face encodings in the current frame of video\n",
    "        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "        face_names = []\n",
    "        for face_encoding in face_encodings:\n",
    "            # See if the face is a match for the known face(s)\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "            name = \"Unknown\"\n",
    "\n",
    "            # Or instead, use the known face with the smallest distance to the new face\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "            best_match_index = np.argmin(face_distances)\n",
    "            if matches[best_match_index]:\n",
    "                name = known_face_names[best_match_index]\n",
    "\n",
    "            face_names.append(name)\n",
    "\n",
    "    process_this_frame = not process_this_frame\n",
    "\n",
    "    # Display the results\n",
    "    for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "        # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "        top *= 4\n",
    "        right *= 4\n",
    "        bottom *= 4\n",
    "        left *= 4\n",
    "\n",
    "        # Draw a box around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5909b9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# coding=utf-8\n",
    "import sys\n",
    "import cv2\n",
    "from PIL import Image,ImageDraw,ImageFont\n",
    "import numpy as np\n",
    "import face_recognition\n",
    " \n",
    "# 加载已知人脸图像\n",
    "known_image = face_recognition.load_image_file(\"wwb.png\")\n",
    " \n",
    "# 提取已知人脸的编码\n",
    "known_face_encoding = face_recognition.face_encodings(known_image)[0]\n",
    " \n",
    "# 初始化摄像头\n",
    "url = \"http://mirror.aarnet.edu.au/pub/TED-talks/911Mothers_2010W-480p.mp4\"#测试视频用\n",
    "video_capture = cv2.VideoCapture(url)\n",
    " \n",
    "def cv2AddChineseText(frame, name, position, fill):\n",
    "    font = ImageFont.truetype('simsun.ttc', 30)\n",
    "    img_pil = Image.fromarray(frame)\n",
    "    draw = ImageDraw.Draw(img_pil)\n",
    "    draw.text(position, name, font=font, fill=fill)\n",
    "    return np.array((img_pil))\n",
    " \n",
    "while True:\n",
    "    # 读取摄像头中的图像\n",
    "    ret, frame = video_capture.read()\n",
    " \n",
    "    # 将图像转换为RGB格式\n",
    "    rgb_frame = frame[:, :, ::-1]\n",
    " \n",
    "    # 检测图像中的人脸\n",
    "    face_locations = face_recognition.face_locations(rgb_frame)\n",
    "    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    " \n",
    "    # 在图像中标记人脸位置\n",
    "    for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
    "        # 判断检测到的人脸是否和已知人脸匹配\n",
    "        matches = face_recognition.compare_faces([known_face_encoding], face_encoding, tolerance=0.38)\n",
    " \n",
    "        # 如果匹配，则标记人脸为已知人脸\n",
    "        name = \"unknow\"\n",
    "        if True in matches:\n",
    "            name = \"know\"\n",
    " \n",
    "        # 在图像中标记人脸位置和姓名\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "        # cv2.putText(frame, name, (left + 6, bottom - 6), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 1)\n",
    "        frame = cv2AddChineseText(frame, name, (left, top - 38), (0, 0, 255))\n",
    "        \n",
    " \n",
    "    # 显示图像\n",
    "    cv2.imshow('Video', frame)\n",
    " \n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de7224f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
